{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from partition import Partition\n",
    "from generators import PartitionClass\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, ConvLSTM2D, BatchNormalization, Conv3D\n",
    "from keras.callbacks.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_partition = training_partitions[12924]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = test_partition._next_oblak_step._next_oblak_step._next_oblak_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 13, 3, 1, 1, 1)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 13, 3, 1, 1, 1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition._next_oblak_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 5, 3, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "oblak = []\n",
    "ar_parts = partition.ar_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_u_chain_start_index = max(ar_parts, key=lambda x: 2 * x + sum(ar_parts[x]))\n",
    "max_u_chain_value = 2 * max_u_chain_start_index + sum(ar_parts[max_u_chain_start_index])\n",
    "\n",
    "if partition[0] >= max_u_chain_value:\n",
    "    oblak.append(partition[0])\n",
    "    partition = Partition(partition[1:])\n",
    "\n",
    "else:\n",
    "    partition = Partition(\n",
    "        tuple([part - 2 for part in partition[:max_u_chain_start_index]])\n",
    "        + partition[max_u_chain_start_index + len(ar_parts[max_u_chain_start_index]) :]\n",
    "    )\n",
    "\n",
    "    oblak.append(max_u_chain_value)\n",
    "\n",
    "ar_parts = partition.ar_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oblak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 13, 5, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_partition.oblak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_partitions = [p for p in PartitionClass(n).partitions]\n",
    "training_matrices = [p.matrix for p in training_partitions]\n",
    "training_durfees = [p._next_oblak_step.matrix for p in training_partitions]\n",
    "\n",
    "X = np.array(training_matrices)\n",
    "y = np.array(training_durfees)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "X_train = X_train.reshape(len(X_train), n, n, 1)\n",
    "X_test = X_test.reshape(len(X_test), n, n, 1)\n",
    "y_train = y_train.reshape(len(y_train), n, n, 1)\n",
    "y_test = y_test.reshape(len(y_test), n, n, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "input_shape=(None, 40, 40, 1),\n",
    "padding=\"same\", return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "padding=\"same\", return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "padding=\"same\", return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "padding=\"same\", return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
    "activation=\"sigmoid\",\n",
    "padding=\"same\", data_format=\"channels_last\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adadelta\", metrics=['accuracy'])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=.0001, patience=300, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "cbs = [es, lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv_lst_m2d_6_input to have 5 dimensions, but got array with shape (28003, 40, 40, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-3303050a8e6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/analytics/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/analytics/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/analytics/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv_lst_m2d_6_input to have 5 dimensions, but got array with shape (28003, 40, 40, 1)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, callbacks=cbs, batch_size=100, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_movies(n_samples=1200, n_frames=15):\n",
    "    row = 80\n",
    "    col = 80\n",
    "    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n",
    "    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1),\n",
    "    dtype=np.float)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Add 3 to 7 moving squares\n",
    "        n = np.random.randint(3, 8)\n",
    "\n",
    "    for j in range(n):\n",
    "        # Initial position\n",
    "        xstart = np.random.randint(20, 60)\n",
    "        ystart = np.random.randint(20, 60)\n",
    "        # Direction of motion\n",
    "        directionx = np.random.randint(0, 3) - 1\n",
    "        directiony = np.random.randint(0, 3) - 1\n",
    "\n",
    "    # Size of the square\n",
    "    w = np.random.randint(2, 4)\n",
    "\n",
    "    for t in range(n_frames):\n",
    "        x_shift = xstart + directionx * t\n",
    "        y_shift = ystart + directiony * t\n",
    "        noisy_movies[i, t, x_shift - w: x_shift + w,\n",
    "        y_shift - w: y_shift + w, 0] += 1\n",
    "\n",
    "    # Make it more robust by adding noise.\n",
    "    # The idea is that if during inference,\n",
    "    # the value of the pixel is not exactly one,\n",
    "    # we need to train the network to be robust and still\n",
    "    # consider it as a pixel belonging to a square.\n",
    "    if np.random.randint(0, 2):\n",
    "        noise_f = (-1)**np.random.randint(0, 2)\n",
    "        noisy_movies[i, t,\n",
    "        x_shift - w - 1: x_shift + w + 1,\n",
    "        y_shift - w - 1: y_shift + w + 1,\n",
    "        0] += noise_f * 0.1\n",
    "\n",
    "    # Shift the ground truth by 1\n",
    "    x_shift = xstart + directionx * (t + 1)\n",
    "    y_shift = ystart + directiony * (t + 1)\n",
    "    shifted_movies[i, t, x_shift - w: x_shift + w,\n",
    "    y_shift - w: y_shift + w, 0] += 1\n",
    "\n",
    "    # Cut to a 40×40 window\n",
    "    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n",
    "    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n",
    "    noisy_movies[noisy_movies >= 1] = 1\n",
    "    shifted_movies[shifted_movies >= 1] = 1\n",
    "    return noisy_movies, shifted_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 15, 40, 40, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_movies(n_samples = 100)[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Next_Oblak_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_partitions = PartitionClass(n).partitions\n",
    "training_matrices = [p.matrix for p in training_partitions]\n",
    "training_durfees = [p._next_oblak_step.matrix for p in training_partitions]\n",
    "\n",
    "X = np.array(training_matrices)\n",
    "y = np.array(training_durfees)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "X_train = X_train.reshape(len(X_train), n, n, 1)\n",
    "X_test = X_test.reshape(len(X_test), n, n, 1)\n",
    "y_train = y_train.reshape(*y_train.shape, -1)\n",
    "y_test = y_test.reshape(*y_test.shape, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(n, n, 1))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "model = Model(input_img, decoded)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(n, n, 1))\n",
    "\n",
    "x = Conv2D(16, (4, 4), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (4, 4), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (4, 4), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (4, 4), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (4, 4), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (4, 4), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "model = Model(input_img, decoded)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=.001, patience=300, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "cbs = [es, lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, callbacks=cbs, batch_size=100, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in PartitionClass(n).partitions:\n",
    "    model_input = np.array(PartitionClass(n).partitions[0].matrix).reshape(-1, *(n, n, 1))\n",
    "    prediction = Partition([x.sum() for x in model.predict(model_input)[0].round() if x.sum()])\n",
    "    \n",
    "    assert partition.oblak == prediction, (partition, partition.oblak, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = [\n",
    "    {\n",
    "        'original': partition,\n",
    "        'oblak': partition.oblak,\n",
    "        'prediction': Partition([x.sum() for x in model.predict(np.array(partition.matrix).reshape(-1, *(n, n, 1)))[0].round() if x.sum()])\n",
    "    }\n",
    " for partition in PartitionClass(n).partitions\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(validation, key = lambda x: x['prediction'])[21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_input = np.array(PartitionClass(n).partitions[0].matrix).reshape(-1, *(n, n, 1))\n",
    "Partition([x.sum() for x in model.predict(model_input)[0].round() if x.sum()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for partition in PartitionClass(n).partitions:\n",
    "    model_input = np.array(partition.matrix).reshape(-1, *(n, n, 1))\n",
    "    prediction = label_binarizer.inverse_transform(model.predict(model_input))[0]\n",
    "    \n",
    "    assert partition.oblak == prediction, (partition.oblak, prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    #add model layers\n",
    "    model.add(Conv2D(64, kernel_size=2, activation=\"relu\", input_shape=(n, n, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=2, activation=\"relu\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=len(y[0]), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_data(n):\n",
    "    training_partitions = PartitionClass(n).partitions\n",
    "    training_matrices = [p.matrix for p in training_partitions]\n",
    "    training_durfees = [p.oblak for p in training_partitions]\n",
    "\n",
    "    X = np.array(training_matrices)\n",
    "    y = np.array(training_durfees)\n",
    "\n",
    "    partition_encoder = dict(zip(set(y), range(len(set(y)))))\n",
    "    y = np.array([partition_encoder[partition] for partition in y])\n",
    "    partition_decoder = {v: k for k, v in partition_encoder.items()}\n",
    "\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    y = label_binarizer.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    X_train = X_train.reshape(*X_train.shape, -1)\n",
    "    X_test = X_test.reshape(*X_test.shape, -1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = build_data(n)\n",
    "\n",
    "model = KerasRegressor(build_fn=base_model)\n",
    "\n",
    "lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=.0001, patience=300, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "cbs = [es, lr]\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, callbacks=cbs, batch_size=100, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perm = PermutationImportance(model, random_state = 1).fit(X_train,y_train)\n",
    "eli5.show_weights(perm, feature_names = X._traincolumns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, b\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = model.input.shape.as_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2027,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cached_property import cached_property\n",
    "from itertools import starmap, zip_longest\n",
    "import operator\n",
    "import numpy as np\n",
    "from itertools import starmap\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.callbacks.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2028,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Partition:\n",
    "    def __init__(self, *args):\n",
    "        args = args or (0,)\n",
    "        if isinstance(args[0], (list, tuple)):\n",
    "            self.parts = tuple(sorted(args[0], reverse = True))  # cast as tuple\n",
    "            \n",
    "        elif isinstance(args[0], (int, float)):\n",
    "            self.parts = tuple(args)\n",
    "            \n",
    "        self.iter_index = -1\n",
    "        self.end = len(self.parts)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.parts)\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Partition):\n",
    "            return self.parts == other.parts\n",
    "        \n",
    "        else:\n",
    "            return self.parts == other\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.parts < other.parts\n",
    "    \n",
    "    def __le__(self, other):\n",
    "        return self.parts <= other.parts\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.parts > other.parts\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        return self.parts >= other.parts\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.parts)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"\\n\".join(\"â˜\"*part for part in self.parts)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other_type = type(other)\n",
    "        \n",
    "        if isinstance(other, (Partition)):\n",
    "            both = zip_longest(self.parts, other.parts, fillvalue = 0)\n",
    "            \n",
    "        elif isinstance(other, (list, tuple)):\n",
    "            both = zip_longest(self.parts, other, fillvalue = 0)\n",
    "            \n",
    "        elif isinstance(other, int):\n",
    "            return Partition((part + other for part in self.parts))\n",
    "        \n",
    "        return Partition(starmap(operator.add, both))\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        return self.parts[ind]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for part in self.parts:\n",
    "            yield part\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.iter_index > self.end: \n",
    "            raise StopIteration \n",
    "            \n",
    "        else: \n",
    "            self.iter_index += 1\n",
    "            return self.parts[self.iter_index]\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(self.parts)\n",
    "        \n",
    "        \n",
    "    def index(self, val, last = False):\n",
    "        for ind, part in enumerate(self[::-1]) if last else enumerate(self):\n",
    "                if part == val:\n",
    "                    return len(self) - ind - 1 if last else ind\n",
    "       \n",
    "        return -1\n",
    "        \n",
    "    @staticmethod\n",
    "    def reverse_insort(a, x, lo=0, hi=None):\n",
    "        \"\"\" Reverse insort version of the bisect.insort method. \"\"\"\n",
    "        if lo < 0:\n",
    "            raise ValueError('lo must be non-negative')\n",
    "        if hi is None:\n",
    "            hi = len(a)\n",
    "        while lo < hi:\n",
    "            mid = (lo+hi)//2\n",
    "            if x > a[mid]: hi = mid\n",
    "            else: lo = mid+1\n",
    "        a.insert(lo, x)\n",
    "    \n",
    "    @cached_property\n",
    "    def is_stable(self):\n",
    "        return not np.where(np.diff(self) > -2)[0].size\n",
    "    \n",
    "    @cached_property\n",
    "    def ar_parts(self):\n",
    "        ar_parts = {}\n",
    "        ar_starts = set()\n",
    "        for ind, part in enumerate(self):\n",
    "            if part in ar_starts:\n",
    "                continue\n",
    "                \n",
    "            end_index = max(Partition(self[ind + 1:]).index(part, last = True), \n",
    "                            Partition(self[ind + 1:]).index(part - 1, last = True))\n",
    "\n",
    "            if end_index >= 0:\n",
    "                ar_parts[ind] = self[ind: ind + end_index + 2]\n",
    "                ar_starts.add(part)\n",
    "                \n",
    "        return ar_parts\n",
    "        \n",
    "    \n",
    "    @cached_property\n",
    "    def box_size(self):\n",
    "        if not self.is_stable:\n",
    "            return 0\n",
    "        \n",
    "        if len(self) == 1:\n",
    "            return self[0]\n",
    "        \n",
    "        return reduce(operator.mul, \n",
    "                      starmap(lambda x, y: x - y - 1, \n",
    "                               list(zip(self.parts, self.parts[1:]))\n",
    "                             )\n",
    "                     ) * self.parts[-1]\n",
    "            \n",
    "    @cached_property\n",
    "    def conjugate(self):\n",
    "        \"\"\" https://en.wikipedia.org/wiki/Partition_(number_theory)#Conjugate_and_self-conjugate_partitions \"\"\"\n",
    "        new_conj = []\n",
    "        parts_copy = list(self.parts)\n",
    "        \n",
    "        while parts_copy:\n",
    "            new_conj.append(len(parts_copy))\n",
    "            parts_copy = [part - 1 for part in parts_copy if part != 1]\n",
    "        \n",
    "        return Partition(new_conj)\n",
    "        \n",
    "    \n",
    "    @cached_property\n",
    "    def matrix(self):\n",
    "        \"\"\" Simply returns a numpy representation of the matrix in a N x N grid where n is the \n",
    "        sum of parts in the given partition instance. \"\"\"\n",
    "        grid = np.zeros((self.sum_of_parts, self.sum_of_parts))\n",
    "        \n",
    "        for ind, part in enumerate(self.parts):\n",
    "            grid[ind][:part] = 1\n",
    "            \n",
    "        return grid\n",
    "    \n",
    "    @cached_property\n",
    "    def sum_of_parts(self):\n",
    "        \"\"\" Returns the sum of parts of the partition instance. \"\"\"\n",
    "        return sum(self.parts)\n",
    "    \n",
    "    @cached_property\n",
    "    def durfee(self):\n",
    "        \"\"\" https://en.wikipedia.org/wiki/Durfee_square \"\"\"\n",
    "        if not self.parts:\n",
    "            return 0\n",
    "        \n",
    "        i = 0\n",
    "        for ind, part in enumerate(self.parts):\n",
    "            if part < ind + 1:\n",
    "                break\n",
    "            i += 1\n",
    "        \n",
    "        return i\n",
    "    \n",
    "    @cached_property\n",
    "    def rank(self):\n",
    "        \"\"\" https://en.wikipedia.org/wiki/Rank_of_a_partition \"\"\"\n",
    "        if not self.parts:\n",
    "            return 0\n",
    "        \n",
    "        return self.parts[0] - len(self.parts)\n",
    "    \n",
    "    @cached_property\n",
    "    def crank(self):\n",
    "        \"\"\" https://en.wikipedia.org/wiki/Crank_of_a_partition \"\"\"\n",
    "        if not self.parts:\n",
    "            return 0\n",
    "        \n",
    "        l = self.parts[0]\n",
    "        w = self.parts.count(1)\n",
    "        \n",
    "        if not w:\n",
    "            return l\n",
    "        \n",
    "        u = len([part for part in self.parts if part > w])\n",
    "        \n",
    "        return u - w\n",
    "        \n",
    "    @cached_property\n",
    "    def rp(self):\n",
    "        \"\"\" https://arxiv.org/pdf/1409.2192.pdf (Denoted as 'r sub p' in the paper). \"\"\"\n",
    "        return (-np.diff(self.parts).clip(-2, 0).sum() // 2) + 1\n",
    "    \n",
    "    def is_almost_rectangular(self):\n",
    "        \"\"\" https://arxiv.org/pdf/1409.2192.pdf \"\"\"\n",
    "        if len(self.parts) < 2:\n",
    "            return False\n",
    "        \n",
    "        return (parts[0] - parts[-1]) <= 1\n",
    "    \n",
    "    \n",
    "    @cached_property\n",
    "    def oblak(self):\n",
    "        \"\"\" https://arxiv.org/pdf/1409.2192.pdf\n",
    "        Returns the resulting partition after performing the partition Recursive Process as specified in the paper. \"\"\"\n",
    "        \n",
    "        oblak = []\n",
    "        partition = Partition(self[::])\n",
    "        \n",
    "        ar_parts = partition.ar_parts\n",
    "        \n",
    "        while ar_parts:\n",
    "            max_u_chain_start_index = max(ar_parts, key = lambda x: 2*x + sum(ar_parts[x]))\n",
    "            max_u_chain_value = 2*max_u_chain_start_index + sum(ar_parts[max_u_chain_start_index])\n",
    "            \n",
    "            if partition[0] >= max_u_chain_value:\n",
    "                oblak.append(partition[0])\n",
    "                partition = Partition(partition[1:])\n",
    "            \n",
    "            else:\n",
    "                partition = Partition(tuple([part - 2 for part in partition[:max_u_chain_start_index]]) + \\\n",
    "                        partition[max_u_chain_start_index + len(ar_parts[max_u_chain_start_index]):])\n",
    "            \n",
    "                oblak.append(max_u_chain_value)\n",
    "                \n",
    "            ar_parts = partition.ar_parts\n",
    "            \n",
    "        for part in partition:\n",
    "            self.reverse_insort(oblak, part)\n",
    "            \n",
    "        return Partition(oblak)\n",
    "    \n",
    "    @cached_property\n",
    "    def _next_oblak_step(self):\n",
    "        \"\"\" This is a property that should only really be used for testing purposes. The only reason this\n",
    "        exists is for neural network training purposes. \"\"\"\n",
    "        \n",
    "        oblak = []\n",
    "        partition = Partition(self[::])\n",
    "        \n",
    "        ar_parts = partition.ar_parts\n",
    "        \n",
    "        if ar_parts:\n",
    "            max_u_chain_start_index = max(ar_parts, key = lambda x: 2*x + sum(ar_parts[x]))\n",
    "            max_u_chain_value = 2*max_u_chain_start_index + sum(ar_parts[max_u_chain_start_index])\n",
    "\n",
    "            if partition[0] >= max_u_chain_value:\n",
    "                oblak.append(partition[0])\n",
    "                partition = Partition(partition[1:])\n",
    "\n",
    "            else:\n",
    "                partition = Partition(tuple([part - 2 for part in partition[:max_u_chain_start_index]]) + \\\n",
    "                        partition[max_u_chain_start_index + len(ar_parts[max_u_chain_start_index]):])\n",
    "\n",
    "                oblak.append(max_u_chain_value)\n",
    "\n",
    "            ar_parts = partition.ar_parts\n",
    "            \n",
    "        for part in partition:\n",
    "            self.reverse_insort(oblak, part)\n",
    "            \n",
    "        return Partition(oblak)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2029,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartitionClass:\n",
    "    def __init__(self, n = 0):\n",
    "        self.n = n\n",
    "         \n",
    "    def _generate_partitions(self):\n",
    "        \"\"\" This function is about as fast as it gets for generating integer partitions\n",
    "        without taking advantage of multithreading. \"\"\"\n",
    "        a = [0 for i in range(self.n + 1)]\n",
    "        k = 1\n",
    "        a[1] = self.n\n",
    "        while k != 0:\n",
    "            x = a[k - 1] + 1\n",
    "            y = a[k] - 1\n",
    "            k -= 1\n",
    "            while x <= y:\n",
    "                a[k] = x\n",
    "                y -= x\n",
    "                k += 1\n",
    "            a[k] = x + y\n",
    "            yield Partition(a[:k + 1])\n",
    "       \n",
    "    @cached_property\n",
    "    def partitions(self):\n",
    "        return [p for p in self._generate_partitions()]\n",
    "            \n",
    "    def filter_by_attribute(self, attr, val = True):\n",
    "        assert hasattr(Partition(), attr), f\"The Partition class does not have attribute: {attr}.\"\n",
    "        \n",
    "        return [p for p in self._generate_partitions() if getattr(p, attr) == val]\n",
    "    \n",
    "    def filter_by_attribute_lists(self, attr, vals):\n",
    "        assert hasattr(Partition(), attr), f\"The Partition class does not have attribute: {attr}.\"\n",
    "        \n",
    "        return [p for p in self._generate_partitions() if getattr(p, attr) in vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2030,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OblakClass(PartitionClass):\n",
    "    def __init__(self, oblak = Partition()):\n",
    "        super().__init__(n = oblak.sum_of_parts)\n",
    "        self.oblak = oblak\n",
    "\n",
    "    @cached_property\n",
    "    def partitions(self):\n",
    "        return [p for p in self._generate_partitions() if p.oblak == self.oblak]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1859,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1994,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_partitions = PartitionClass(n).partitions\n",
    "training_matrices = [p.matrix for p in training_partitions]\n",
    "training_durfees = [p.oblak for p in training_partitions]\n",
    "\n",
    "X = np.array(training_matrices)\n",
    "y = np.array(training_durfees)\n",
    "\n",
    "partition_encoder = dict(zip(set(y), range(len(set(y)))))\n",
    "y = np.array([partition_encoder[partition] for partition in y])\n",
    "partition_decoder = {v: k for k, v in partition_encoder.items()}\n",
    "\n",
    "label_binarizer = LabelBinarizer()\n",
    "y = label_binarizer.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "X_train = X_train.reshape(len(X_train), n, n, 1)\n",
    "X_test = X_test.reshape(len(X_test), n, n, 1)\n",
    "# y_train = y_train.reshape(*y_train.shape, -1)\n",
    "# y_test = y_test.reshape(*y_test.shape, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1997,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1925,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=2, activation=\"relu\", input_shape=(n, n, 1)))\n",
    "model.add(Conv2D(32, kernel_size=2, activation=\"relu\"))\n",
    "model.add(Conv2D(16, kernel_size=2, activation=\"relu\"))\n",
    "model.add(Conv2D(8, kernel_size=2,  activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=len(y[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1998,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=2, activation=\"relu\", input_shape=(n, n, 1)))\n",
    "model.add(Conv2D(32, kernel_size=2, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=len(y[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1999,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2000,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=.0001, patience=300, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "cbs = [es, lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2001,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4203 samples, validate on 1401 samples\n",
      "Epoch 1/1000\n",
      "4203/4203 [==============================] - 7s 2ms/step - loss: 4.4621 - accuracy: 0.0293 - val_loss: 4.1829 - val_accuracy: 0.0557\n",
      "Epoch 2/1000\n",
      "2400/4203 [================>.............] - ETA: 2s - loss: 3.8860 - accuracy: 0.0929"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2001-3303050a8e6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, callbacks=cbs, batch_size=100, workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Next_Oblak_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2041,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2062,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_partitions = PartitionClass(n).partitions\n",
    "training_matrices = [p.matrix for p in training_partitions]\n",
    "training_durfees = [p._next_oblak_step.matrix for p in training_partitions]\n",
    "\n",
    "X = np.array(training_matrices)\n",
    "y = np.array(training_durfees)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "X_train = X_train.reshape(len(X_train), n, n, 1)\n",
    "X_test = X_test.reshape(len(X_test), n, n, 1)\n",
    "y_train = y_train.reshape(*y_train.shape, -1)\n",
    "y_test = y_test.reshape(*y_test.shape, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2083,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_662 (Conv2D)          (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_210 (MaxPoolin (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_663 (Conv2D)          (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_211 (MaxPoolin (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_664 (Conv2D)          (None, 7, 7, 8)           584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_212 (MaxPoolin (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_665 (Conv2D)          (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_183 (UpSamplin (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_666 (Conv2D)          (None, 8, 8, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_184 (UpSamplin (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_667 (Conv2D)          (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_185 (UpSamplin (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_668 (Conv2D)          (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(n, n, 1))\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "model = Model(input_img, decoded)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2091,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_718 (Conv2D)          (None, 28, 28, 16)        272       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_234 (MaxPoolin (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_719 (Conv2D)          (None, 14, 14, 8)         2056      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_235 (MaxPoolin (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_720 (Conv2D)          (None, 7, 7, 8)           1032      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_236 (MaxPoolin (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_721 (Conv2D)          (None, 4, 4, 8)           1032      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_207 (UpSamplin (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_722 (Conv2D)          (None, 8, 8, 8)           1032      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_208 (UpSamplin (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_723 (Conv2D)          (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_209 (UpSamplin (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_724 (Conv2D)          (None, 28, 28, 1)         257       \n",
      "=================================================================\n",
      "Total params: 6,849\n",
      "Trainable params: 6,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=(n, n, 1))\n",
    "\n",
    "x = Conv2D(16, (4, 4), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (4, 4), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (4, 4), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (4, 4), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (4, 4), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (4, 4), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "model = Model(input_img, decoded)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2092,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2093,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=.001, patience=300, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "cbs = [es, lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2095,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2788 samples, validate on 930 samples\n",
      "Epoch 1/100\n",
      "2788/2788 [==============================] - 4s 2ms/step - loss: 0.2576 - accuracy: 0.9626 - val_loss: 0.0534 - val_accuracy: 0.9681\n",
      "Epoch 2/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0496 - accuracy: 0.9787 - val_loss: 0.0402 - val_accuracy: 0.9811\n",
      "Epoch 3/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0358 - accuracy: 0.9853 - val_loss: 0.0334 - val_accuracy: 0.9864\n",
      "Epoch 4/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0324 - accuracy: 0.9863 - val_loss: 0.0254 - val_accuracy: 0.9894\n",
      "Epoch 5/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0271 - accuracy: 0.9885 - val_loss: 0.0266 - val_accuracy: 0.9891\n",
      "Epoch 6/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0258 - accuracy: 0.9889 - val_loss: 0.0251 - val_accuracy: 0.9892\n",
      "Epoch 7/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0253 - accuracy: 0.9891 - val_loss: 0.0248 - val_accuracy: 0.9892\n",
      "Epoch 8/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0250 - accuracy: 0.9891 - val_loss: 0.0252 - val_accuracy: 0.9890\n",
      "Epoch 9/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0245 - accuracy: 0.9893 - val_loss: 0.0246 - val_accuracy: 0.9894\n",
      "Epoch 10/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0239 - accuracy: 0.9895 - val_loss: 0.0256 - val_accuracy: 0.9893\n",
      "Epoch 11/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0245 - accuracy: 0.9892 - val_loss: 0.0238 - val_accuracy: 0.9895\n",
      "Epoch 12/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0239 - accuracy: 0.9895 - val_loss: 0.0236 - val_accuracy: 0.9896\n",
      "Epoch 13/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0238 - accuracy: 0.9894 - val_loss: 0.0245 - val_accuracy: 0.9896\n",
      "Epoch 14/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0239 - accuracy: 0.9894 - val_loss: 0.0250 - val_accuracy: 0.9894\n",
      "Epoch 15/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0237 - accuracy: 0.9895 - val_loss: 0.0258 - val_accuracy: 0.9888\n",
      "Epoch 16/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0234 - accuracy: 0.9896 - val_loss: 0.0239 - val_accuracy: 0.9897\n",
      "Epoch 17/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0235 - accuracy: 0.9895 - val_loss: 0.0239 - val_accuracy: 0.9897\n",
      "Epoch 18/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0223 - accuracy: 0.9901 - val_loss: 0.0226 - val_accuracy: 0.9900\n",
      "Epoch 19/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0221 - accuracy: 0.9902 - val_loss: 0.0225 - val_accuracy: 0.9900\n",
      "Epoch 20/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0221 - accuracy: 0.9901 - val_loss: 0.0224 - val_accuracy: 0.9900\n",
      "Epoch 21/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0220 - accuracy: 0.9902 - val_loss: 0.0224 - val_accuracy: 0.9899\n",
      "Epoch 22/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0220 - accuracy: 0.9902 - val_loss: 0.0223 - val_accuracy: 0.9899\n",
      "Epoch 23/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0220 - accuracy: 0.9901 - val_loss: 0.0223 - val_accuracy: 0.9899\n",
      "Epoch 24/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0219 - accuracy: 0.9902 - val_loss: 0.0223 - val_accuracy: 0.9901\n",
      "Epoch 25/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0219 - accuracy: 0.9902 - val_loss: 0.0222 - val_accuracy: 0.9900\n",
      "Epoch 26/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0218 - accuracy: 0.9902 - val_loss: 0.0222 - val_accuracy: 0.9901\n",
      "Epoch 27/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0218 - accuracy: 0.9902 - val_loss: 0.0221 - val_accuracy: 0.9900\n",
      "Epoch 28/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0218 - accuracy: 0.9903 - val_loss: 0.0221 - val_accuracy: 0.9903\n",
      "Epoch 29/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0217 - accuracy: 0.9903 - val_loss: 0.0221 - val_accuracy: 0.9901\n",
      "Epoch 30/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0217 - accuracy: 0.9903 - val_loss: 0.0220 - val_accuracy: 0.9902\n",
      "Epoch 31/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0216 - accuracy: 0.9903 - val_loss: 0.0220 - val_accuracy: 0.9901\n",
      "Epoch 32/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0216 - accuracy: 0.9903 - val_loss: 0.0220 - val_accuracy: 0.9902\n",
      "Epoch 33/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0216 - accuracy: 0.9904 - val_loss: 0.0219 - val_accuracy: 0.9902\n",
      "Epoch 34/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0216 - accuracy: 0.9903 - val_loss: 0.0218 - val_accuracy: 0.9901\n",
      "Epoch 35/100\n",
      "2788/2788 [==============================] - 4s 1ms/step - loss: 0.0215 - accuracy: 0.9904 - val_loss: 0.0218 - val_accuracy: 0.9902\n",
      "Epoch 36/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0215 - accuracy: 0.9904 - val_loss: 0.0218 - val_accuracy: 0.9903\n",
      "Epoch 37/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0215 - accuracy: 0.9904 - val_loss: 0.0217 - val_accuracy: 0.9902\n",
      "Epoch 38/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0214 - accuracy: 0.9904 - val_loss: 0.0217 - val_accuracy: 0.9903\n",
      "Epoch 39/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0214 - accuracy: 0.9905 - val_loss: 0.0217 - val_accuracy: 0.9903\n",
      "Epoch 40/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0213 - accuracy: 0.9905 - val_loss: 0.0216 - val_accuracy: 0.9903\n",
      "Epoch 41/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0213 - accuracy: 0.9905 - val_loss: 0.0216 - val_accuracy: 0.9904\n",
      "Epoch 42/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0212 - accuracy: 0.9906 - val_loss: 0.0215 - val_accuracy: 0.9905\n",
      "Epoch 43/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0212 - accuracy: 0.9906 - val_loss: 0.0215 - val_accuracy: 0.9905\n",
      "Epoch 44/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0212 - accuracy: 0.9906 - val_loss: 0.0215 - val_accuracy: 0.9904\n",
      "Epoch 45/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0211 - accuracy: 0.9906 - val_loss: 0.0214 - val_accuracy: 0.9906\n",
      "Epoch 46/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0211 - accuracy: 0.9906 - val_loss: 0.0213 - val_accuracy: 0.9906\n",
      "Epoch 47/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0211 - accuracy: 0.9907 - val_loss: 0.0213 - val_accuracy: 0.9906\n",
      "Epoch 48/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0210 - accuracy: 0.9907 - val_loss: 0.0212 - val_accuracy: 0.9906\n",
      "Epoch 49/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0209 - accuracy: 0.9907 - val_loss: 0.0212 - val_accuracy: 0.9906\n",
      "Epoch 50/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0209 - accuracy: 0.9907 - val_loss: 0.0211 - val_accuracy: 0.9907\n",
      "Epoch 51/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0209 - accuracy: 0.9908 - val_loss: 0.0211 - val_accuracy: 0.9905\n",
      "Epoch 52/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0208 - accuracy: 0.9908 - val_loss: 0.0210 - val_accuracy: 0.9906\n",
      "Epoch 53/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0208 - accuracy: 0.9908 - val_loss: 0.0210 - val_accuracy: 0.9908\n",
      "Epoch 54/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0207 - accuracy: 0.9909 - val_loss: 0.0209 - val_accuracy: 0.9908\n",
      "Epoch 55/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0207 - accuracy: 0.9909 - val_loss: 0.0209 - val_accuracy: 0.9908\n",
      "Epoch 56/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0206 - accuracy: 0.9910 - val_loss: 0.0208 - val_accuracy: 0.9909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0205 - accuracy: 0.9910 - val_loss: 0.0207 - val_accuracy: 0.9910\n",
      "Epoch 58/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0205 - accuracy: 0.9910 - val_loss: 0.0207 - val_accuracy: 0.9910\n",
      "Epoch 59/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0204 - accuracy: 0.9911 - val_loss: 0.0206 - val_accuracy: 0.9911\n",
      "Epoch 60/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0203 - accuracy: 0.9911 - val_loss: 0.0206 - val_accuracy: 0.9911\n",
      "Epoch 61/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0203 - accuracy: 0.9912 - val_loss: 0.0205 - val_accuracy: 0.9911\n",
      "Epoch 62/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0202 - accuracy: 0.9912 - val_loss: 0.0204 - val_accuracy: 0.9911\n",
      "Epoch 63/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0201 - accuracy: 0.9912 - val_loss: 0.0203 - val_accuracy: 0.9912\n",
      "Epoch 64/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0201 - accuracy: 0.9913 - val_loss: 0.0202 - val_accuracy: 0.9913\n",
      "Epoch 65/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0200 - accuracy: 0.9914 - val_loss: 0.0202 - val_accuracy: 0.9913\n",
      "Epoch 66/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0199 - accuracy: 0.9914 - val_loss: 0.0201 - val_accuracy: 0.9913\n",
      "Epoch 67/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0198 - accuracy: 0.9914 - val_loss: 0.0200 - val_accuracy: 0.9914\n",
      "Epoch 68/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0197 - accuracy: 0.9915 - val_loss: 0.0200 - val_accuracy: 0.9913\n",
      "Epoch 69/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0197 - accuracy: 0.9915 - val_loss: 0.0199 - val_accuracy: 0.9914\n",
      "Epoch 70/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0196 - accuracy: 0.9916 - val_loss: 0.0197 - val_accuracy: 0.9916\n",
      "Epoch 71/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0195 - accuracy: 0.9917 - val_loss: 0.0196 - val_accuracy: 0.9916\n",
      "Epoch 72/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0194 - accuracy: 0.9917 - val_loss: 0.0195 - val_accuracy: 0.9917\n",
      "Epoch 73/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0193 - accuracy: 0.9918 - val_loss: 0.0194 - val_accuracy: 0.9917\n",
      "Epoch 74/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0192 - accuracy: 0.9918 - val_loss: 0.0193 - val_accuracy: 0.9918\n",
      "Epoch 75/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0191 - accuracy: 0.9919 - val_loss: 0.0192 - val_accuracy: 0.9919\n",
      "Epoch 76/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0190 - accuracy: 0.9919 - val_loss: 0.0191 - val_accuracy: 0.9919\n",
      "Epoch 77/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0190 - accuracy: 0.9919 - val_loss: 0.0191 - val_accuracy: 0.9920\n",
      "Epoch 78/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0188 - accuracy: 0.9920 - val_loss: 0.0190 - val_accuracy: 0.9920\n",
      "Epoch 79/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0188 - accuracy: 0.9920 - val_loss: 0.0189 - val_accuracy: 0.9920\n",
      "Epoch 80/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0187 - accuracy: 0.9921 - val_loss: 0.0188 - val_accuracy: 0.9921\n",
      "Epoch 81/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0186 - accuracy: 0.9921 - val_loss: 0.0188 - val_accuracy: 0.9921\n",
      "Epoch 82/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0185 - accuracy: 0.9922 - val_loss: 0.0185 - val_accuracy: 0.9922\n",
      "Epoch 83/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0185 - accuracy: 0.9922 - val_loss: 0.0184 - val_accuracy: 0.9923\n",
      "Epoch 84/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0184 - accuracy: 0.9922 - val_loss: 0.0184 - val_accuracy: 0.9922\n",
      "Epoch 85/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0183 - accuracy: 0.9923 - val_loss: 0.0183 - val_accuracy: 0.9923\n",
      "Epoch 86/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0182 - accuracy: 0.9923 - val_loss: 0.0182 - val_accuracy: 0.9923\n",
      "Epoch 87/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0181 - accuracy: 0.9923 - val_loss: 0.0181 - val_accuracy: 0.9923\n",
      "Epoch 88/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0180 - accuracy: 0.9923 - val_loss: 0.0180 - val_accuracy: 0.9924\n",
      "Epoch 89/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0180 - accuracy: 0.9923 - val_loss: 0.0182 - val_accuracy: 0.9923\n",
      "Epoch 90/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0179 - accuracy: 0.9924 - val_loss: 0.0179 - val_accuracy: 0.9924\n",
      "Epoch 91/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0180 - accuracy: 0.9924 - val_loss: 0.0181 - val_accuracy: 0.9923\n",
      "Epoch 92/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0178 - accuracy: 0.9924 - val_loss: 0.0179 - val_accuracy: 0.9923\n",
      "Epoch 93/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0178 - accuracy: 0.9924 - val_loss: 0.0178 - val_accuracy: 0.9925\n",
      "Epoch 94/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0177 - accuracy: 0.9925 - val_loss: 0.0177 - val_accuracy: 0.9925\n",
      "Epoch 95/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0176 - accuracy: 0.9925 - val_loss: 0.0178 - val_accuracy: 0.9925\n",
      "Epoch 96/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0176 - accuracy: 0.9925 - val_loss: 0.0176 - val_accuracy: 0.9926\n",
      "Epoch 97/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0176 - accuracy: 0.9925 - val_loss: 0.0177 - val_accuracy: 0.9925\n",
      "Epoch 98/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0177 - accuracy: 0.9924 - val_loss: 0.0176 - val_accuracy: 0.9926\n",
      "Epoch 99/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0174 - accuracy: 0.9926 - val_loss: 0.0174 - val_accuracy: 0.9927\n",
      "Epoch 100/100\n",
      "2788/2788 [==============================] - 3s 1ms/step - loss: 0.0174 - accuracy: 0.9926 - val_loss: 0.0175 - val_accuracy: 0.9926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15d737c88>"
      ]
     },
     "execution_count": 2095,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, callbacks=cbs, batch_size=100, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2096,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "((1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), (28,), (22.0, 3.0, 1.0))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2096-9cc21b871ba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moblak\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moblak\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: ((1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1), (28,), (22.0, 3.0, 1.0))"
     ]
    }
   ],
   "source": [
    "for partition in PartitionClass(n).partitions:\n",
    "    model_input = np.array(PartitionClass(n).partitions[0].matrix).reshape(-1, *(n, n, 1))\n",
    "    prediction = Partition([x.sum() for x in model.predict(model_input)[0].round() if x.sum()])\n",
    "    \n",
    "    assert partition.oblak == prediction, (partition, partition.oblak, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2099,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = [\n",
    "    {\n",
    "        'original': partition,\n",
    "        'oblak': partition.oblak,\n",
    "        'prediction': Partition([x.sum() for x in model.predict(np.array(partition.matrix).reshape(-1, *(n, n, 1)))[0].round() if x.sum()])\n",
    "    }\n",
    " for partition in PartitionClass(n).partitions\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': (10, 9, 9), 'oblak': (28,), 'prediction': (15.0, 5.0, 2.0, 1.0)}"
      ]
     },
     "execution_count": 2105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(validation, key = lambda x: x['prediction'])[21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2049,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.013035430572926998, 0.9945550560951233]"
      ]
     },
     "execution_count": 2049,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2059,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.0, 1.0)"
      ]
     },
     "execution_count": 2059,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = np.array(PartitionClass(n).partitions[0].matrix).reshape(-1, *(n, n, 1))\n",
    "Partition([x.sum() for x in model.predict(model_input)[0].round() if x.sum()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2050,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2050-1f0709d9703b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moblak\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moblak\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "for partition in PartitionClass(n).partitions:\n",
    "    model_input = np.array(partition.matrix).reshape(-1, *(n, n, 1))\n",
    "    prediction = label_binarizer.inverse_transform(model.predict(model_input))[0]\n",
    "    \n",
    "    assert partition.oblak == prediction, (partition.oblak, prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2002,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2003,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    #add model layers\n",
    "    model.add(Conv2D(64, kernel_size=2, activation=\"relu\", input_shape=(n, n, 1)))\n",
    "    model.add(Conv2D(32, kernel_size=2, activation=\"relu\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=len(y[0]), activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_data(n):\n",
    "    training_partitions = PartitionClass(n).partitions\n",
    "    training_matrices = [p.matrix for p in training_partitions]\n",
    "    training_durfees = [p.oblak for p in training_partitions]\n",
    "\n",
    "    X = np.array(training_matrices)\n",
    "    y = np.array(training_durfees)\n",
    "\n",
    "    partition_encoder = dict(zip(set(y), range(len(set(y)))))\n",
    "    y = np.array([partition_encoder[partition] for partition in y])\n",
    "    partition_decoder = {v: k for k, v in partition_encoder.items()}\n",
    "\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    y = label_binarizer.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    X_train = X_train.reshape(*X_train.shape, -1)\n",
    "    X_test = X_test.reshape(*X_test.shape, -1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = build_data(n)\n",
    "\n",
    "model = KerasRegressor(build_fn=base_model)\n",
    "\n",
    "lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=.0001, patience=300, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "cbs = [es, lr]\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, callbacks=cbs, batch_size=100, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2019,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 4. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2019-f2f691c26055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPermutationImportance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traincolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/eli5/sklearn/permutation_importance.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prefit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 539\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 4. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "perm = PermutationImportance(model, random_state = 1).fit(X_train,y_train)\n",
    "eli5.show_weights(perm, feature_names = X._traincolumns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2010,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasRegressor' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2010-895c27fcae32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasRegressor' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "model.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1971,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.wrappers.scikit_learn.KerasRegressor at 0x178202048>"
      ]
     },
     "execution_count": 1971,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, b\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1957,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = model.input.shape.as_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
